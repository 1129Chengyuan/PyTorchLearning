{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPvTcFt3JJCjvhBOh5MLaDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1129Chengyuan/PyTorchLearning/blob/main/TensorOperations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZmEz0jDRJ3AF"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_torch = torch.arange(10)\n",
        "print(my_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GU5etGpJ6uv",
        "outputId": "77be4db4-9be7-4301-85bc-3749d34ac5a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape and View\n",
        "my_torch = my_torch.reshape(2,5) #If the input size is wrong it will throw an error\n",
        "print(my_torch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqkXiX6VKHFW",
        "outputId": "e207b724-c99f-42f9-f624-d7ba3a7aa46b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape if we don't know the number of items\n",
        "my_torch2 = torch.arange(10)\n",
        "print(my_torch2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEKEzS9lKVKX",
        "outputId": "04a66ddd-0c6b-430c-f7fc-63852bcc1bff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_torch2 = my_torch2.reshape(2, -1) #use -1 to denote an unknown value!\n",
        "# Note: Even if you use -1, a rectangular shape has to be preserved\n",
        "print(my_torch2)\n",
        "my_torch2 = my_torch2.reshape(-1, 5)\n",
        "print(my_torch2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QTh51umKb6s",
        "outputId": "800e4c1d-29f2-4578-f66c-53e458a45c84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n",
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_torch3 = torch.arange(10)\n",
        "print(my_torch3)\n",
        "my_torch4 = my_torch3.view(2,5)\n",
        "print(my_torch4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCXHVHhIK3Om",
        "outputId": "6399f76f-4237-49da-ba00-654f67c4605f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So what's the difference?\n",
        "While it seems like both *view* and *reshape* return a manipulated version of the object, how they handle tensors depends on whether or not it is contiguous...\n",
        "\n",
        "A contiguous tensor is a tensor whose values are stored in a single, uninterrupted piece of memory. On the other hand, a non-contiguous tensor may have gaps in its memory layout.\n",
        "\n",
        "* If applied to a contiguous tensor, both *view* and *reshape* produces a new \"view\" of the given tensor's memory. In other words, they interpret the contiguous tensor as an alternate layout - a piece of memory that stores 16 values, for example, could be viewed as a 4x4 element 2D tensor instead.\n",
        "\n",
        "* If applied to a non-contiguous tensor, the *reshape* method produces a duplicate of the piece of memory, returning a tensor whose memory is not shared with the given tensor. On the other hand, the *view* method throws a *RunTime Error*.\n",
        "\n",
        "It is important to keep in mind that view objects created reference the data in the original, so any changes would be reflected in the *view* and *reshape* objects!"
      ],
      "metadata": {
        "id": "5Kxii-4HL3O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import arange\n",
        "\n",
        "contiguous = arange(16).view(4, 4)             # Create contiguous 4×4 tensor\n",
        "noncontiguous = arange(20).view(4, 5)[:, :4]   # Create non-contiguous 4×4 tensor\n",
        "\n",
        "contiguous_r = contiguous.reshape(16)          # OK: produces a 1-d view\n",
        "assert contiguous_r.data_ptr() == contiguous.data_ptr()  # Same memory used\n",
        "\n",
        "contiguous_v = contiguous.view(16)             # OK: produces a 1-d view\n",
        "assert contiguous_v.data_ptr() == contiguous.data_ptr()  # Same memory used\n",
        "\n",
        "noncontiguous_r = noncontiguous.reshape(16)    # OK: produces a new 1-d array\n",
        "assert noncontiguous_r.data_ptr() != noncontiguous.data_ptr()  # New memory used\n",
        "\n",
        "noncontiguous_v = noncontiguous.view(16)       # ERROR: cannot produce view"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "RTpnRinhM_BS",
        "outputId": "6bb0b27d-b852-4db0-85ad-d87c3459cc77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82fe26e23d9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnoncontiguous_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnoncontiguous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# New memory used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnoncontiguous_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoncontiguous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# ERROR: cannot produce view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BlUf236mNLlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changes will be reflected!\n",
        "my_torch5 = torch.arange(10)\n",
        "print(my_torch5)\n",
        "my_torch6 = my_torch5.reshape(2,5)\n",
        "print(my_torch6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfMoDlIzNZOV",
        "outputId": "f510928d-dcca-4d3f-f9d1-85a29e57dd18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "tensor([[0, 1, 2, 3, 4],\n",
            "        [5, 6, 7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_torch5[1] = 4141\n",
        "print(my_torch5)\n",
        "print(my_torch6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUOodH-lNsnd",
        "outputId": "cdb0a59c-6c99-4fdd-ef80-a3ffa497d4f3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   0, 4141,    2,    3,    4,    5,    6,    7,    8,    9])\n",
            "tensor([[   0, 4141,    2,    3,    4],\n",
            "        [   5,    6,    7,    8,    9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Slices"
      ],
      "metadata": {
        "id": "ja51YsTMN-qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_torch7 = torch.arange(10)\n",
        "print(my_torch7)\n",
        "# Grabbing a specific item\n",
        "print(my_torch7[7]) # Output: tensor(7)\n",
        "# Grab a slice\n",
        "my_torch8 = my_torch7.reshape(5,2)\n",
        "print(my_torch8)\n",
        "print(my_torch8[:,1]) # Output: tensor([1, 3, 5, 7, 9])\n",
        "print(my_torch8[:,1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44MQuI5CN_Yz",
        "outputId": "13540f26-debd-4b64-e93d-8e3456977f43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "tensor(7)\n",
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7],\n",
            "        [8, 9]])\n",
            "tensor([[2, 3],\n",
            "        [4, 5],\n",
            "        [6, 7],\n",
            "        [8, 9]])\n",
            "tensor([[1],\n",
            "        [3],\n",
            "        [5],\n",
            "        [7],\n",
            "        [9]])\n"
          ]
        }
      ]
    }
  ]
}